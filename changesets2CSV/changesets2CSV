#!/usr/bin/env python3
'''
changesets2CSV

This tool will build out a CSV of changeset info queried based on the given parameters

Copyright (c) 2019 Kaart Group <admin@kaartgroup.com>

Released under the MIT license: http://opensource.org/licenses/mit-license.php

'''

import xml.etree.ElementTree as ET
import csv
import argparse
import requests
from cachecontrol import CacheControl
from datetime import datetime, date, timedelta
import os
import sys
import glob
import re
import time
import ssl
from xlsxwriter.workbook import Workbook
import json

TEST_OVERPASS_USER_AGENT = "trackDownload/0.1 (lucas.bingham@kaartgroup.com)"
'''
with open('test.json') as f:
    TEST_JSON_DICT = json.load(f)
    print(TEST_JSON_DICT)
'''



#{'tags':[{'tag':'name','const':'highway'},{'tag':'surface','const':'highway'}],'users':[{'user_id':'9320902','name':'Traaker_L'},{'user_id':'8600365','name':'spuddy93'}]}


def valid_date(s):
    try:
        return datetime.strptime(s, "%Y-%m-%d")
    except ValueError:
        msg = "Not a valid date: '{0}'.".format(s)
        raise argparse.ArgumentTypeError(msg)


def create_specific(args):
    out = os.path.join(args.output, args.user + '.csv')
    changesets = get_changesets(args.user, args.start_time, args.end_time, args.bbox)
    if len(changesets) < 1:
        sys.exit(1)
    changeset_csv(out, changesets)
    if args.excel:
        create_excel_file


def create_summary(args):
    changeset_total = 0
    edit_total = 0
    disc_total = 0
    epc_total = 0  # edits per changeset
    add_total = 0
    mod_total = 0
    delete_total = 0
    summary = []

    tag_counts = []
    #each entry willl be a dictionary with <tag_combo>_added/modified/deleted:0

    with open(args.input_file, mode='r') as f:
        loaded_dict = json.load(f)
        global json_dict
        json_dict = loaded_dict
        users = json_dict['users']

        #print(users)
        #for user in users:
            #print(user['name'])
            #print(user['user_id'])

        for user in users:
            print("Processing changesets for {} ...".format(str(user['name'])), end="\r")
            out_file = os.path.join(args.output, str(user['name']) + '.csv')
            changesets = get_changesets(user['user_id'], args.start_time, args.end_time, args.bbox)

            # no point in doing anything if there aren't any changesets
            if len(changesets) < 1:
                print("Warning: There were no changesets for {} within those parameters\n".format(str(user['name'])))
                continue

            counts = changeset_csv(out_file, changesets, name=str(user['name']), summary=True)
            counts['Editor'] = str(user['name'])
            summary.append(counts)
            changeset_total += counts['Changesets']
            edit_total += counts['Edits']
            disc_total += counts['Discussions']
            add_total += counts['Additions']
            mod_total += counts['Modifications']
            delete_total += counts['Deletions']
            #TODO: Add tag-specific counts here

            time.sleep(10)
            print("Processing changesets for {} ... Done\n".format(str(user['name'])))


    try:
        epc_total = round((edit_total / changeset_total), 2)


        summary.append({
            'Editor': 'TOTAL',
            'Changesets': changeset_total,
            'Edits': edit_total,
            'Discussions': disc_total,
            'Edits/Changeset': epc_total,
            'Additions': add_total,
            'Modifications': mod_total,
            'Deletions': delete_total
        })
        summary_csv(args, os.path.join(args.output, "summary.csv"), summary)
    except ZeroDivisionError:
       print("Warning: There were no changesets within those parameters")


def summary_csv(args, output_file, summary):
    with open(output_file, 'w') as f:
        fieldnames = ['Editor', 'Changesets', 'Edits', 'Discussions', 'Edits/Changeset', 'Additions', 'Modifications', 'Deletions']
        csv_writer = csv.DictWriter(f, fieldnames=fieldnames)
        csv_writer.writeheader()

        csv_writer.writerows(summary)

    if args.excel:
        create_excel_file(args, os.path.dirname(os.path.realpath(output_file)))


def create_weekly(args):
    today = date.today()
    weekday = today.weekday()
    start_delta = timedelta(days=weekday, weeks=1)
    start = today - start_delta
    end = start + timedelta(days=5)
    args.start_time = str(start)
    args.end_time = str(end)
    create_summary(args)

def count_new_modified_deleted(changeset):
    """Get the new, modified, or deleted objects in a changeset
    >>> count_new_modified_deleted(get_changesets(user="vorpalblade", start_time="2019-07-07", end_time="2019-07-08")[0].get('id'))
    {'Added': 129, 'Modified': 5, 'Deleted': 8}
    """
    api_url = "https://www.openstreetmap.org/api/0.6/changeset/{changeset}/download".format(changeset=changeset)
    session = CacheControl(requests.session())
    result = session.get(api_url).text
    root = ET.fromstring(result)
    newModifiedDeleted = {}
    newModifiedDeleted['Added'] = len(root.findall('create'))
    newModifiedDeleted['Modified'] = len(root.findall('modify'))
    newModifiedDeleted['Deleted'] = len(root.findall('delete'))
    return newModifiedDeleted

def overpass_status(api_status_url = "https://overpass-api.de/api/status"):
    """Get the overpass status -- this returns an int with the time to wait"""
    session = requests.session()
    session.headers.update({'User-Agent': TEST_OVERPASS_USER_AGENT})
    cached_session = CacheControl(session)
    response = cached_session.get(api_status_url)
    if (response.status_code != requests.codes.ok):
        raise ValueError("Bad Request: {}".format(api_status_url))
    parsed_response = {'wait_time': []}
    for i in response.text.splitlines():
        if "Connected as" in i:
            parsed_response['connected_as'] = i.split(":")[1].strip()
        elif "Current time" in i:
            parsed_response['current_time'] = i.split(":")[1].strip()
        elif "Rate limit" in i:
            parsed_response['rate_limit'] = int(i.split(":")[1].strip())
        elif "slots available now" in i:
            parsed_response['slots_available'] = int(i.split(" ")[0].strip())
        elif "Slot available after" in i:
            parsed_response['wait_time'].append(int(i.split(" ")[5]))
    if 'slots_available' not in parsed_response:
        parsed_response['slots_available'] = 0
    wait_time = 0
    if parsed_response['rate_limit'] - parsed_response['slots_available'] >= 2 and len(parsed_response['wait_time']) > 0:
        return max(parsed_response['wait_time'])
    return wait_time

def overpass_query(query):
    """Query the overpass servers. This may block for extended periods of time, depending upon the query"""

    session = requests.session()
    session.headers.update({'User-Agent': TEST_OVERPASS_USER_AGENT})
    cached_session = CacheControl(session)
    response = cached_session.post("http://overpass-api.de/api/interpreter", data={'data': query})
    wait_time = overpass_status()
    loop = 0
    while (wait_time > 0):
        time.sleep(wait_time)
        wait_time = overpass_status()
        loop += 1
    while (response.status_code == requests.codes.too_many_requests):
        time.sleep(10)
        response = cached_session.post("http://overpass-api.de/api/interpreter", data={'data': query})
    if (response.status_code != requests.codes.ok):
        print("Bad request")
        print(response.text)
        print(response.status_code)
        raise ValueError("Bad Request: {}".format(query))

    xml = response.text

    if (response.status_code != requests.codes.ok):
        raise ValueError("We got a bad response code of {} for {} which resulted in:\r\n{}".format(response.status_code, query, xml))
    content_type = response.headers.get('content-type')
    if content_type == 'application/osm3s+xml':
        return ET.ElementTree(ElementTree.fromstring(xml))
    elif content_type == 'application/json':
        return response.json()
    else:
        raise ValueError("Unexpected content type ({}) from the query: {}".format(content_type, query))

def count_tag_change(changesets,info_json,osm_obj_type='*'):
    tags_to_check = info_json['tags']

    #Testing variables
    print_version_lists = False
    object_limit_for_query=0
    print_query = False
    dont_run_query = False
    use_hardcoded_query_result = False
    print_query_response = False
    dont_process_query = False
    #changesets = {<changeset_id>:{<tag_to_check>:[<objects>]}}
    objects_by_changeset = {}

    #Get all objects touched in each changeset
    #We go by changeset, then by tag
    for changeset in changesets:
        #New changeset in list
        objects_by_changeset[changeset] = {} #?
        #Request XML for each changeset
        api_url = "https://www.openstreetmap.org/api/0.6/changeset/{changeset}/download".format(changeset=changeset)
        dev_api_url = "https://master.apis.dev.openstreetmap.org/api/0.6/changeset/{changeset}/download".format(changeset=changeset)

        session = CacheControl(requests.session())
        result = session.get(api_url).text
        root = ET.fromstring(result)

        #If we are looking for a constant tag
        for this_tag in tags_to_check:
            check_tag = this_tag['tag']

            const_tag = this_tag['const']
            objects_by_changeset[changeset][check_tag+'_'+const_tag] = []

            if const_tag != "none":
                #Retrieve all objects that have the tags we're looking for
                objs_modified = root.findall("./modify/{osm_obj_type}/tag[@k='{const_tag}']..".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
                objs_created = root.findall("./create/{osm_obj_type}/tag[@k='{const_tag}']..".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
                objs_deleted = root.findall("./delete/{osm_obj_type}/tag[@k='{const_tag}']..".format(const_tag=const_tag,osm_obj_type=osm_obj_type))

                #Store each modified object's data in our list, new_ver_objects, as dictionaries
                for obj in objs_modified:
                    this_obj = {"id":obj.attrib['id'],"version":int(obj.attrib['version'])}
                    tag_elements = obj.findall("tag")
                    for tag_element in tag_elements:
                        this_obj[tag_element.attrib['k']] = tag_element.attrib['v']
                    objects_by_changeset[changeset][check_tag+'_'+const_tag].append(this_obj)

        #If we only care about the tag being changed
            else:
                #Retrieve all objects that have the tags we're looking for
                objs_modified = root.findall("./modify/{osm_obj_type}".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
                objs_created = root.findall("./create/{osm_obj_type}".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
                objs_deleted = root.findall("./delete/{osm_obj_type}".format(const_tag=const_tag,osm_obj_type=osm_obj_type))

                #Store each modified object's data in our list, new_ver_objects, as dictionaries
                for obj in objs_modified:
                    this_obj = {"id":obj.attrib['id'],"version":int(obj.attrib['version'])}
                    these_tags = obj.findall("tag")
                    for tag_element in tag_elements:
                        this_obj[tag_element.attrib['k']] = tag_element.attrib['v']
                    objects_by_changeset[changeset][check_tag+'_'+const_tag].append(this_obj)


    change_count_by_changeset = {}

    #Count number of objects per changeset
    for set in objects_by_changeset:
        change_count_by_changeset[set] = {}
        for tag in objects_by_changeset[set]:
            change_count_by_changeset[set][tag] = len(objects_by_changeset[set][tag])

    #4Testing: print objects and data in new_ver_objects list
    if print_version_lists:
        print("New_Ver: ")
        print(objects_by_changeset)
        print()

    #Build query to get previous versions of all objects in new_ver_objects
    #Start of Overpass Query
    #We will go by changeset, then by tag
    #[<changeset>][<tag>]
    query = "[out:json][timeout:25];"
    query_count = 0
    for this_set in objects_by_changeset:
        for this_tag in objects_by_changeset[this_set]:
            #Build each query part for each object
            if (query_count < object_limit_for_query or object_limit_for_query == 0):
                for obj in objects_by_changeset[this_set][this_tag]:
                    #print(obj)
                    #>>>Prints dictionary of object{'id':<value>,'version':<value>,<keys and values for each tag afterwards}
                    #Can this ever happen?
                    if obj["version"] > 1 and (query_count < object_limit_for_query or object_limit_for_query == 0):
                        if object_limit_for_query != 0:
                            print("Object ",query_count+1," of ",object_limit_for_query)

                        query_part = "timeline({osm_obj_type}, {osm_id}, {prev_version}); for (t['created']) {{ retro(_.val) {{ {osm_obj_type}(id:{osm_id}); out meta;}} }}"\
                        .format(osm_obj_type = osm_obj_type, osm_id = obj["id"],prev_version = int(obj["version"])-1)

                        query += query_part
                        query_count += 1

    #4Testing: print out the query and/or response
    if print_query:
        print('Query: ')
        print(query)

    if dont_run_query == False:
        #Submit the query
        query_json = overpass_query(query)

    if print_query_response:
        if dont_run_query:
            print("Cannot print query response if it was not run")
        else:
            print(query_json)


    if (dont_process_query == False and dont_run_query == False)\
    or use_hardcoded_query_result:
        #4Testing: use a canned query_result from a string
        if use_hardcoded_query_result:
            query_json = HARDCODED_QUERY_RESULT
        #Dictionaries of id, value, version
        old_objects_by_changeset = {}
        #old_objects_by_changeset{<changeset_id>:{<tag_to_check>:[<objects>]}}
        #We go by changeset, then tag
        #We will index tags inside of indexing changesets
        current_set_index = 0
        current_tag_index = 0
        #print(tags_to_check)
        #>> [{'tag': 'name', 'const': 'highway'}, {'tag': 'name', 'const': 'waterway'}]
        objects_added = 0

        old_objects_by_changeset={}

        #Iterate through query result elements
        old_version_objects = []
        for element in query_json["elements"]:
            this_obj = this_obj = {"id":element['id'],"version":element['version']}
            for key, value in element['tags'].items():
                this_obj[key] = value
            old_version_objects.append(this_obj)

        #Sort list of objects by changeset and tag
        old_obj_index = 0
        for set in objects_by_changeset:
            old_objects_by_changeset[set] = {}
            objects_by_tag = objects_by_changeset[set]
            for tag in objects_by_tag:
                old_objects_by_changeset[set][tag] = []
                #objects_by_tag[tag] gives us the list of objects that we need to check
                #We can iterate through this list to get the objects
                for this_obj in objects_by_tag[tag]:
                    #Iterate through tags in the query response json
                    if old_obj_index < object_limit_for_query or object_limit_for_query == 0:
                        obj_to_add = old_version_objects[old_obj_index]
                        old_obj_index += 1
                        old_objects_by_changeset[set][tag].append(obj_to_add)



        #4Testing: Print list of old-version objects
        if print_version_lists:
            print("Old_Ver: ")
            print(old_objects_by_changeset)
            print()

        #See what values changed
        changes_by_changeset = {}
        #Initialize change counts for all sets
        for this_changeset in old_objects_by_changeset:
            #print(this_changeset)
            #>>> changeset id
            changes_by_changeset[this_changeset] = {}
            for this_tag in old_objects_by_changeset[this_changeset]:
                #print(this_tag)
                #tag + '_' + const_tag e.g. name_highway
                add_key = this_tag +' added'
                modify_key = this_tag + ' modified'
                delete_key = this_tag + ' deleted'
                changes_by_changeset[this_changeset][this_tag] = {add_key:0,modify_key:0,delete_key:0}



        differences_counted = 0
        compare_index = 0
        tag_index = 0
        for this_changeset in old_objects_by_changeset:
            tag_index = 0
            for this_tag in old_objects_by_changeset[this_changeset]:
                compare_index = 0
                for this_obj in old_objects_by_changeset[this_changeset][this_tag]:
                    this_tag_to_check = tags_to_check[tag_index]['tag']

                    old_obj = old_objects_by_changeset[this_changeset][this_tag][compare_index]
                    new_obj = objects_by_changeset[this_changeset][this_tag][compare_index]

                    if str(old_obj['id']) != new_obj['id']:
                        print('ERROR: ID MISTMATCH')

                    old_val = None
                    if old_obj.get(this_tag_to_check,False):
                        old_val = old_obj[this_tag_to_check]

                    new_val = None
                    if new_obj.get(this_tag_to_check,False):
                        new_val = new_obj[this_tag_to_check]

                    if old_val == None:
                        if new_val != None:
                            if old_val != new_val:
                                print(new_val,' added')
                                changes_by_changeset[this_changeset][this_tag][this_tag +' added'] += 1
                            else:
                                print(old_val," didn't change")
                    else:
                        if new_val != None:
                            if old_val != new_val:
                                print(old_val,' changed to ',new_val)
                                changes_by_changeset[this_changeset][this_tag][this_tag +' modified'] += 1
                            else:
                                print(old_val," didn't change")
                        else:
                            if old_val != new_val:
                                print(old_val,' deleted')
                                changes_by_changeset[this_changeset][this_tag][this_tag +' deleted'] += 1
                            else:
                                print(old_val," didn't change")




                    compare_index += 1

                tag_index += 1

        return changes_by_changeset
    else:
        return {'added':0,'modified':0,'deleted':0}

def get_changesets(user=None, start_time=None, end_time=None, bbox=None):
    """Get the changesets for a user between start_time and end_time with no bbox
    >>> len(get_changesets(user="vorpalblade", start_time="2019-07-07", end_time="2019-07-08"))
    1
    >>> len(get_changesets(user="9019988", start_time="2018-11-05", end_time="2018-11-06"))
    149
    >>> len(get_changesets(user="vorpalblade", start_time=[datetime(2019, 7, 7)], end_time="2019-07-08"))
    1
    >>> len(get_changesets(user="9019988", start_time="2018-11-05", end_time=[datetime(2018, 11, 6)]))
    149
    >>> len(get_changesets(user="9019988", start_time=[datetime(2018, 11, 5)], end_time=[datetime(2018, 11, 6)]))
    149
    """
    query_params = {}
    if user:
        if user.isdigit():
            query_params['user'] = user
        else:
            query_params['display_name'] = user

    if start_time and end_time:
        if type(start_time) is list and len(start_time) == 1:
            start_time = start_time[0].strftime("%Y-%m-%d")
        if type(end_time) is list and len(end_time) == 1:
            end_time = end_time[0].strftime("%Y-%m-%d")
        query_params['time'] = ','.join([start_time, end_time])

    if bbox:
        query_params['bbox'] = ','.join(bbox)

    changesets = []

    try:
        api_url = "https://api.openstreetmap.org/api/0.6/changesets"
        session = CacheControl(requests.session())
        result = session.get(api_url, params=query_params)
        root = ET.fromstring(result.text)
        sets = root.findall('changeset')
        changesets.extend(sets)

        dateFormat = '%Y-%m-%dT%H:%M:%SZ'
        while len(sets) >= 100:
            new_end = datetime.strptime(sets[-1].get('closed_at'), dateFormat) - timedelta(0,5)
            new_end = new_end.strftime(dateFormat)
            start_time = "1970-01-01" if not start_time else start_time
            query_params['time'] = ','.join([start_time, new_end])
            result = session.get(api_url, params=query_params).text
            root = ET.fromstring(result)
            sets = root.findall('changeset')
            changesets.extend(sets)

    except Exception as e:
        print("Error with calling the API: " + str(e))

    return changesets


def changeset_csv(output_file, changesets, name=None, summary=False):
    try:
        with open(output_file, 'w') as f:
            fieldnames = ['Username', 'ID', 'Comment', 'Open', 'Created at', 'Closed at', 'Changes', 'Added', 'Modified', 'Deleted']
            tag_fieldnames = []
            #Add fields for
            #TEST_JSON_DICT['tags'] = [{'tag':'name','const':'highway'},{'tag':'surface','const':'highway'}]
            for tag in json_dict['tags']:
                prefix = tag['tag']+'_'+tag['const']
                for suffix in [' added',' modified',' deleted']:
                    fieldnames.append(prefix + suffix)
                    tag_fieldnames.append(prefix + suffix)

            fieldnames.extend(['Discussions', 'URL'])
            csv_writer = csv.DictWriter(f, fieldnames=fieldnames)
            url = "https://www.openstreetmap.org/changeset/{}"

            csv_writer.writeheader()

            # Initialize counters for summary
            changeset_count = 0
            edit_count = 0
            add_count = 0
            modify_count = 0
            delete_count = 0
            discussion_count = 0

            #Count additions, changes, and deletions that are names
            changeset_ids = []
            for set in changesets:
                changeset_ids.append(set.get('id'))

            tag_changes = count_tag_change(changeset_ids,json_dict,"way")

            for item in changesets:
                changeset = {'Username': item.get('user'),
                             'ID': item.get('id'),
                             'Comment': '',
                             'Open': item.get('open'),
                             'Created at': item.get('created_at'),
                             'Closed at': item.get('closed_at'),
                             'Changes': item.get('changes_count'),
                             'Discussions': item.get('comments_count'),
                             'URL': url.format(item.get('id'))}

                for child in item:
                    if child.get('k') == 'comment':
                        changeset['Comment'] = child.get('v').encode('utf-8')

                #General adds, modifies, deletes
                changesetInformation = count_new_modified_deleted(changeset['ID'])
                for key in changesetInformation:
                    changeset[key] = changesetInformation[key]

                #Tag-specific adds, modifies, deletes
                #print(item.get('id'))
                these_tag_changes = tag_changes[item.get('id')]
                for this_tag in these_tag_changes:
                    for change_type, change_count in these_tag_changes[this_tag].items():
                        changeset[change_type] = change_count
                '''
                for key in these_tag_changes:
                    key_string = field_prefixes[0] + ' ' + key
                    #print(key_string)
                    changeset[key_string] = these_tag_changes[key]
                '''



                changeset_count += 1
                add_count += changeset['Added']
                modify_count += changeset['Modified']
                delete_count += changeset['Deleted']
                edit_count += int(item.get('changes_count'))
                discussion_count += int(item.get('comments_count'))

                csv_writer.writerow(changeset)


    except Exception as e:
        print("Error creating csv {}".format(e))
        raise

    if summary:
        summary_dict = {'Changesets': changeset_count,
                'Edits': edit_count,
                'Additions': add_count,
                'Modifications': modify_count,
                'Deletions': delete_count,
                'Discussions': discussion_count,
                'Edits/Changeset': round((edit_count/changeset_count), 2)}

        return summary_dict

    return True


def get_args():
    parser = argparse.ArgumentParser(usage='''changesets2CSV [-h] [-b min_lon min_lat max_lon max_lat] [-v] <command>
    ''', description="Commands for creating changeset CSV's")
    parser.add_argument('-t', '--test', help="Run tests to make certain that everything works", action='store_true')
    parser.add_argument('--bbox', nargs=4,
                        help="The bbox to query changesets. Values separated by spaces.",
                        metavar=('min_lon', 'min_lat', 'max_lon', 'max_lat'))
    # parser.add_argument('-v', '--verbose', action='store_true') #nothing to verbose yet
    parser.add_argument('-o', '--output', help="Location to create .csv files (default is current location)", default=os.getcwd())
    parser.add_argument('-x', '--excel', help="Create a .xlsx file.", action='store_true')
    parser.set_defaults(which='main')
    subparsers = parser.add_subparsers(title='commands')

    specific = subparsers.add_parser("specific", help='Specific query', description="Run query with specific parameters and create one output file.")
    specific.add_argument('-u', '--user', help="The OSM username or user id to use for the query (either username or user id, NOT both).")
    specific.add_argument('-s', '--start_time', nargs=1, type=valid_date, help="The start time of the window to query (YYYY-MM-DD).")
    specific.add_argument('-e', '--end_time', nargs=1, type=valid_date, help="The end time of the window to query (YYYY-MM-DD).")
    specific.set_defaults(func=create_specific, which="specific")

    summary = subparsers.add_parser("summary", help="Create a summary of changesets", description="Create a summary for a specified time range.")
    #summary.add_argument('users', help="Path of the .config file of users.")
    summary.add_argument('input_file',help="Path of the .json file of users and tags")
    summary.add_argument('-s', '--start_time', nargs=1, type=valid_date, help="The start time of the window to query (YYYY-MM-DD).")
    summary.add_argument('-e', '--end_time', nargs=1, type=valid_date, help="The end time of the window to query (YYYY-MM-DD).")
    summary.set_defaults(func=create_summary, which="summary")

    weekly = subparsers.add_parser("weekly", help="Create a weekly summary of changesets")
    #weekly.add_argument('users', help="Path of the .config file of users.")
    weekly.add_argument('input_file',help="Path of the .json file of users and tags")
    weekly.set_defaults(func=create_weekly, which="weekly")

    args = parser.parse_args()

    try:
        if args.test:
            import doctest
            doctest.testmod()
            return
        if args.which == 'specific':
            if not ((args.user or args.bbox) or (args.start_time and args.end_time)):
                parser.error('No query parameters supplied: add --user or --bbox, or --start_time and --end_time.')

        args.func(args)

    except AttributeError:
        parser.print_help(sys.stderr)


def create_excel_file(args, output_dir):
    files = sorted(glob.glob(output_dir + os.sep + '*' + '.csv'))
    if args.start_time and args.end_time:
        name = "_".join([str(args.start_time), str(args.end_time)]) + '.xlsx'
    else:
        name = 'Kaart_activity.xlsx'
    workbook = Workbook(os.path.join(output_dir, name), {'strings_to_numbers': True, 'constant_memory': True})
    for csvFile in files:
        name = csvFile.replace('.csv', '').replace(output_dir + os.sep, '')
        worksheet = workbook.add_worksheet(name)
        with open(csvFile, 'r') as csvfile:
            csvreader = csv.reader(csvfile, delimiter=',')
            for row_index, row in enumerate(csvreader):
                for col_index, data in enumerate(row):
                    worksheet.write(row_index, col_index, data)
        # TODO: add chart
        # if name == 'summary':
        #     chart = workbook.add_chart({'type': 'column'})
        # chart.add_series({
        #     'name':
        # })
    workbook.close()


def parse(item):
    try:
        newItem = float(item)
        return newItem
    except ValueError:
        pass
    try:
        booleans = ["true", "false"]
        if item.lower() in booleans:
            newItem = bool(item)
            return newItem
    except ValueError:
        pass
    try:
        if item.startswith("b'"):
            newItem = item.replace("b'", '', 1)
            newItem = re.sub(r"(.*)'", r'\1', newItem)
            newItem = bytes(newItem, "ascii").decode("utf-8")
            # Not perfected yet
            # return newItem
    except (ValueError, TypeError):
        pass
    return item

json_dict = {}

if __name__ == "__main__":

    ''' Vamanos '''
    get_args()
