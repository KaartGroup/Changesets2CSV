#!/usr/bin/env python3
'''
changesets2CSV

This tool will build out a CSV of changeset info queried based on the given parameters

Copyright (c) 2019 Kaart Group <admin@kaartgroup.com>

Released under the MIT license: http://opensource.org/licenses/mit-license.php

'''

import xml.etree.ElementTree as ET
import csv
import argparse
import requests
from cachecontrol import CacheControl
from datetime import datetime, date, timedelta
import os
import sys
import glob
import re
import time
import ssl
from xlsxwriter.workbook import Workbook

TEST_OVERPASS_USER_AGENT = "trackDownload/0.1 (lucas.bingham@kaartgroup.com)"

def valid_date(s):
    try:
        return datetime.strptime(s, "%Y-%m-%d")
    except ValueError:
        msg = "Not a valid date: '{0}'.".format(s)
        raise argparse.ArgumentTypeError(msg)


def create_specific(args):
    out = os.path.join(args.output, args.user + '.csv')
    changesets = get_changesets(args.user, args.start_time, args.end_time, args.bbox)
    if len(changesets) < 1:
        sys.exit(1)
    changeset_csv(out, changesets)
    if args.excel:
        create_excel_file


def create_summary(args):
    changeset_total = 0
    edit_total = 0
    disc_total = 0
    epc_total = 0  # edits per changeset
    summary = []
    with open(args.users, mode='r') as f:
        users = csv.DictReader(f)
        for user in users:
            print("Processing changesets for {} ...".format(str(user['name'])), end="\r")
            out_file = os.path.join(args.output, str(user['name']) + '.csv')
            changesets = get_changesets(user['user_id'], args.start_time, args.end_time, args.bbox)

            # no point in doing anything if there aren't any changesets
            if len(changesets) < 1:
                print("Warning: There were no changesets for {} within those parameters\n".format(str(user['name'])))
                continue

            counts = changeset_csv(out_file, changesets, name=str(user['name']), summary=True)
            counts['Editor'] = str(user['name'])
            summary.append(counts)
            changeset_total += counts['Changesets']
            edit_total += counts['Edits']
            disc_total += counts['Discussions']
            time.sleep(10)
            print("Processing changesets for {} ... Done\n".format(str(user['name'])))

    try:
        epc_total = round((edit_total / changeset_total), 2)
        summary.append({
            'Editor': 'TOTAL',
            'Changesets': changeset_total,
            'Edits': edit_total,
            'Discussions': disc_total,
            'Edits/Changeset': epc_total
        })
        summary_csv(args, os.path.join(args.output, "summary.csv"), summary)
    except ZeroDivisionError:
       print("Warning: There were no changesets within those parameters")


def summary_csv(args, output_file, summary):
    with open(output_file, 'w') as f:
        fieldnames = ['Editor', 'Changesets', 'Edits', 'Discussions', 'Edits/Changeset', 'Additions', 'Modifications', 'Deletions']
        csv_writer = csv.DictWriter(f, fieldnames=fieldnames)
        csv_writer.writeheader()

        csv_writer.writerows(summary)

    if args.excel:
        create_excel_file(args, os.path.dirname(os.path.realpath(output_file)))


def create_weekly(args):
    today = date.today()
    weekday = today.weekday()
    start_delta = timedelta(days=weekday, weeks=1)
    start = today - start_delta
    end = start + timedelta(days=5)
    args.start_time = str(start)
    args.end_time = str(end)
    create_summary(args)

def count_new_modified_deleted(changeset):
    """Get the new, modified, or deleted objects in a changeset
    >>> count_new_modified_deleted(get_changesets(user="vorpalblade", start_time="2019-07-07", end_time="2019-07-08")[0].get('id'))
    {'Added': 129, 'Modified': 5, 'Deleted': 8}
    """
    api_url = "https://www.openstreetmap.org/api/0.6/changeset/{changeset}/download".format(changeset=changeset)
    session = CacheControl(requests.session())
    result = session.get(api_url).text
    root = ET.fromstring(result)
    newModifiedDeleted = {}
    newModifiedDeleted['Added'] = len(root.findall('create'))
    newModifiedDeleted['Modified'] = len(root.findall('modify'))
    newModifiedDeleted['Deleted'] = len(root.findall('delete'))
    return newModifiedDeleted

def overpass_status(api_status_url = "https://overpass-api.de/api/status"):
    """Get the overpass status -- this returns an int with the time to wait"""
    session = requests.session()
    session.headers.update({'User-Agent': TEST_OVERPASS_USER_AGENT})
    cached_session = CacheControl(session)
    response = cached_session.get(api_status_url)
    if (response.status_code != requests.codes.ok):
        raise ValueError("Bad Request: {}".format(api_status_url))
    parsed_response = {'wait_time': []}
    for i in response.text.splitlines():
        if "Connected as" in i:
            parsed_response['connected_as'] = i.split(":")[1].strip()
        elif "Current time" in i:
            parsed_response['current_time'] = i.split(":")[1].strip()
        elif "Rate limit" in i:
            parsed_response['rate_limit'] = int(i.split(":")[1].strip())
        elif "slots available now" in i:
            parsed_response['slots_available'] = int(i.split(" ")[0].strip())
        elif "Slot available after" in i:
            parsed_response['wait_time'].append(int(i.split(" ")[5]))
    if 'slots_available' not in parsed_response:
        parsed_response['slots_available'] = 0
    wait_time = 0
    if parsed_response['rate_limit'] - parsed_response['slots_available'] >= 2 and len(parsed_response['wait_time']) > 0:
        return max(parsed_response['wait_time'])
    return wait_time

def overpass_query(query):
    """Query the overpass servers. This may block for extended periods of time, depending upon the query"""

    session = requests.session()
    session.headers.update({'User-Agent': TEST_OVERPASS_USER_AGENT})
    cached_session = CacheControl(session)
    response = cached_session.post("http://overpass-api.de/api/interpreter", data={'data': query})
    wait_time = overpass_status()
    loop = 0
    while (wait_time > 0):
        time.sleep(wait_time)
        wait_time = overpass_status()
        loop += 1
    while (response.status_code == requests.codes.too_many_requests):
        time.sleep(10)
        response = cached_session.post("http://overpass-api.de/api/interpreter", data={'data': query})
    if (response.status_code != requests.codes.ok):
        print("Bad request")
        print(response.text)
        print(response.status_code)
        raise ValueError("Bad Request: {}".format(query))

    xml = response.text

    if (response.status_code != requests.codes.ok):
        raise ValueError("We got a bad response code of {} for {} which resulted in:\r\n{}".format(response.status_code, query, xml))
    content_type = response.headers.get('content-type')
    if content_type == 'application/osm3s+xml':
        return ET.ElementTree(ElementTree.fromstring(xml))
    elif content_type == 'application/json':
        return response.json()
    else:
        raise ValueError("Unexpected content type ({}) from the query: {}".format(content_type, query))

def count_tag_change(changesets,tag, osm_obj_type="*",const_tag="none"):
  #Testing variables
  print_version_lists = True
  print_query = False
  print_query_response = False
  object_limit_for_query=0
  dont_run_query = False
  dont_process_query = False
  #Dictionaries of id, value, version
  new_ver_objects = []

  for changeset in changesets:
      api_url = "https://www.openstreetmap.org/api/0.6/changeset/{changeset}/download".format(changeset=changeset)
      dev_api_url = "https://master.apis.dev.openstreetmap.org/api/0.6/changeset/{changeset}/download".format(changeset=changeset)
      api_way_url = "https://www.openstreetmap.org/api/0.6/way/"
      api_url = api_url
      session = CacheControl(requests.session())
      result = session.get(api_url).text
      root = ET.fromstring(result)


      if const_tag != "none":
          objs_modified = root.findall("./modify/{osm_obj_type}/tag[@k='{const_tag}']..".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
          objs_created = root.findall("./create/{osm_obj_type}/tag[@k='{const_tag}']..".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
          objs_deleted = root.findall("./delete/{osm_obj_type}/tag[@k='{const_tag}']..".format(const_tag=const_tag,osm_obj_type=osm_obj_type))

          #print(len(objs_created)," created. ",len(objs_deleted)," deleted.",len(objs_modified)," modified.")

          for obj in objs_modified:
              #print("way ",obj.attrib["id"])
              this_obj = {"id":obj.attrib['id'],"version":int(obj.attrib['version'])}
              tags = obj.findall("tag")
              for thisTag in tags:
                  this_obj[thisTag.attrib['k']] = thisTag.attrib['v']

              new_ver_objects.append(this_obj)

      else:
          objs_modified = root.findall("./modify/{osm_obj_type}".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
          objs_created = root.findall("./create/{osm_obj_type}".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
          objs_deleted = root.findall("./delete/{osm_obj_type}".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
          for obj in objs_modified:
              #print("way ",obj.attrib["id"])
              this_obj = {"id":obj.attrib['id'],"version":int(obj.attrib['version'])}
              tags = obj.findall("tag")
              for tag in tags:
                  this_obj[tag.attrib['k']] = tag.attrib['v']

              new_ver_objects.append(this_obj)


  if print_version_lists:
      print("New_Ver: ")
      for obj in new_ver_objects: print(obj)
      print()

  query = "[out:json][timeout:25];"
  query_count = 0
  for obj in new_ver_objects:
      #Can this ever happen?
      if obj["version"] > 1 and (query_count < object_limit_for_query or object_limit_for_query == 0):
          if object_limit_for_query != 0:
              print("Object ",query_count+1," of ",object_limit_for_query)
          query_part = "timeline({osm_obj_type}, {osm_id}, {prev_version}); for (t['created']) {{ retro(_.val) {{ {osm_obj_type}(id:{osm_id}); out meta;}} }}"\
          .format(osm_obj_type = osm_obj_type, osm_id = obj["id"],prev_version = int(obj["version"])-1)
          query += query_part
          query_count += 1
  if print_query: print(query)

  if dont_run_query == False:
      query_json = overpass_query(query)

  if print_query_response:
      if dont_run_query:
          print("Cannot print query if it was not run")
      else:
          print(query_json)

  if dont_process_query == False and dont_run_query == False:
      #Dictionaries of id, value, version
      old_ver_objects = []
      for element in query_json["elements"]:
          these_tags = element['tags']
          if these_tags.get(tag,None) != None:
              old_ver_objects.append({"id":element['id'],'value':element['tags'][tag],"version":element['version']})
          else:
              old_ver_objects.append({"id":element['id'],'value':None,"version":element['version']})
      if print_version_lists:
          print("Old_Ver: ")
          for obj in old_ver_objects: print(obj)
          print()

      #See what values changed
      changes = {'added':0,'modified':0,'deleted':0}
      for i in range(len(old_ver_objects)):
          old_value = old_ver_objects[i]["value"]
          if new_ver_objects[i].get(tag,False):
              new_value = new_ver_objects[i][tag]
          else:
              new_value = None

          if old_value == None:
              if new_value != None:
                  print('add')
                  changes['added'] += 1
          else:
              if new_value != None:
                  print('change')
                  changes['modified'] += 1
              else:
                  print('delete')
                  changes['deleted'] += 1

          if old_value != new_value:
              print(old_value," became ",new_value)
          else:
              print(old_value, "didn't change")

      return changes
  else:
      return {"testing"}

def get_changesets(user=None, start_time=None, end_time=None, bbox=None):
    """Get the changesets for a user between start_time and end_time with no bbox
    >>> len(get_changesets(user="vorpalblade", start_time="2019-07-07", end_time="2019-07-08"))
    1
    >>> len(get_changesets(user="9019988", start_time="2018-11-05", end_time="2018-11-06"))
    149
    >>> len(get_changesets(user="vorpalblade", start_time=[datetime(2019, 7, 7)], end_time="2019-07-08"))
    1
    >>> len(get_changesets(user="9019988", start_time="2018-11-05", end_time=[datetime(2018, 11, 6)]))
    149
    >>> len(get_changesets(user="9019988", start_time=[datetime(2018, 11, 5)], end_time=[datetime(2018, 11, 6)]))
    149
    """
    query_params = {}
    if user:
        if user.isdigit():
            query_params['user'] = user
        else:
            query_params['display_name'] = user

    if start_time and end_time:
        if type(start_time) is list and len(start_time) == 1:
            start_time = start_time[0].strftime("%Y-%m-%d")
        if type(end_time) is list and len(end_time) == 1:
            end_time = end_time[0].strftime("%Y-%m-%d")
        query_params['time'] = ','.join([start_time, end_time])

    if bbox:
        query_params['bbox'] = ','.join(bbox)

    changesets = []

    try:
        api_url = "https://api.openstreetmap.org/api/0.6/changesets"
        session = CacheControl(requests.session())
        result = session.get(api_url, params=query_params)
        root = ET.fromstring(result.text)
        sets = root.findall('changeset')
        changesets.extend(sets)

        dateFormat = '%Y-%m-%dT%H:%M:%SZ'
        while len(sets) >= 100:
            new_end = datetime.strptime(sets[-1].get('closed_at'), dateFormat) - timedelta(0,5)
            new_end = new_end.strftime(dateFormat)
            start_time = "1970-01-01" if not start_time else start_time
            query_params['time'] = ','.join([start_time, new_end])
            result = session.get(api_url, params=query_params).text
            root = ET.fromstring(result)
            sets = root.findall('changeset')
            changesets.extend(sets)

    except Exception as e:
        print("Error with calling the API: " + str(e))

    return changesets


def changeset_csv(output_file, changesets, name=None, summary=False):
    try:
        with open(output_file, 'w') as f:
            fieldnames = ['Username', 'ID', 'Comment', 'Open', 'Created at', 'Closed at', 'Changes', 'Added', 'Modified', 'Deleted', 'Discussions', 'URL']
            csv_writer = csv.DictWriter(f, fieldnames=fieldnames)
            url = "https://www.openstreetmap.org/changeset/{}"

            csv_writer.writeheader()

            # Initialize counters for summary
            changeset_count = 0
            edit_count = 0
            add_count = 0
            modify_count = 0
            delete_count = 0
            discussion_count = 0

            changeset_ids = []
            for set in changesets:
                changeset_ids.append(set.get('id'))
            name_changes = count_tag_change(changeset_ids,"name", "way", "highway")
            print(name_changes)


            for item in changesets:
                changeset = {'Username': item.get('user'),
                             'ID': item.get('id'),
                             'Comment': '',
                             'Open': item.get('open'),
                             'Created at': item.get('created_at'),
                             'Closed at': item.get('closed_at'),
                             'Changes': item.get('changes_count'),
                             'Discussions': item.get('comments_count'),
                             'URL': url.format(item.get('id'))}

                for child in item:
                    if child.get('k') == 'comment':
                        changeset['Comment'] = child.get('v').encode('utf-8')

                changesetInformation = count_new_modified_deleted(changeset['ID'])
                for key in changesetInformation:
                    changeset[key] = changesetInformation[key]

                changeset_count += 1
                add_count += changeset['Added']
                modify_count += changeset['Modified']
                delete_count += changeset['Deleted']
                edit_count += int(item.get('changes_count'))
                discussion_count += int(item.get('comments_count'))

                csv_writer.writerow(changeset)


            testset = {'Username': '',
                         'ID': '',
                         'Comment': '',
                         'Open': '',
                         'Created at': '',
                         'Closed at': '',
                         'Changes': '',
                         'Added':"Names " +str(name_changes['added']),
                         'Modified':"Names " +str(name_changes['modified']),
                         'Deleted':"Names " +str(name_changes['deleted']),
                         'Discussions': '',
                         'URL': ''}
            csv_writer.writerow(testset)
    except Exception as e:
        print("Error creating csv {}".format(e))
        raise

    if summary:
        return {'Changesets': changeset_count,
                'Edits': edit_count,
                'Additions': add_count,
                'Modifications': modify_count,
                'Deletions': delete_count,
                'Discussions': discussion_count,
                'Edits/Changeset': round((edit_count/changeset_count), 2)}

    return True


def get_args():
    parser = argparse.ArgumentParser(usage='''changesets2CSV [-h] [-b min_lon min_lat max_lon max_lat] [-v] <command>
    ''', description="Commands for creating changeset CSV's")
    parser.add_argument('-t', '--test', help="Run tests to make certain that everything works", action='store_true')
    parser.add_argument('--bbox', nargs=4,
                        help="The bbox to query changesets. Values separated by spaces.",
                        metavar=('min_lon', 'min_lat', 'max_lon', 'max_lat'))
    # parser.add_argument('-v', '--verbose', action='store_true') #nothing to verbose yet
    parser.add_argument('-o', '--output', help="Location to create .csv files (default is current location)", default=os.getcwd())
    parser.add_argument('-x', '--excel', help="Create a .xlsx file.", action='store_true')
    parser.set_defaults(which='main')
    subparsers = parser.add_subparsers(title='commands')

    specific = subparsers.add_parser("specific", help='Specific query', description="Run query with specific parameters and create one output file.")
    specific.add_argument('-u', '--user', help="The OSM username or user id to use for the query (either username or user id, NOT both).")
    specific.add_argument('-s', '--start_time', nargs=1, type=valid_date, help="The start time of the window to query (YYYY-MM-DD).")
    specific.add_argument('-e', '--end_time', nargs=1, type=valid_date, help="The end time of the window to query (YYYY-MM-DD).")
    specific.set_defaults(func=create_specific, which="specific")

    summary = subparsers.add_parser("summary", help="Create a summary of changesets", description="Create a summary for a specified time range.")
    summary.add_argument('users', help="Path of the .config file of users.")
    summary.add_argument('-s', '--start_time', nargs=1, type=valid_date, help="The start time of the window to query (YYYY-MM-DD).")
    summary.add_argument('-e', '--end_time', nargs=1, type=valid_date, help="The end time of the window to query (YYYY-MM-DD).")
    summary.set_defaults(func=create_summary, which="summary")

    weekly = subparsers.add_parser("weekly", help="Create a weekly summary of changesets")
    weekly.add_argument('users', help="Path of the .config file of users.")
    weekly.set_defaults(func=create_weekly, which="weekly")

    args = parser.parse_args()

    try:
        if args.test:
            import doctest
            doctest.testmod()
            return
        if args.which == 'specific':
            if not ((args.user or args.bbox) or (args.start_time and args.end_time)):
                parser.error('No query parameters supplied: add --user or --bbox, or --start_time and --end_time.')

        args.func(args)

    except AttributeError:
        parser.print_help(sys.stderr)


def create_excel_file(args, output_dir):
    files = sorted(glob.glob(output_dir + os.sep + '*' + '.csv'))
    if args.start_time and args.end_time:
        name = "_".join([str(args.start_time), str(args.end_time)]) + '.xlsx'
    else:
        name = 'Kaart_activity.xlsx'
    workbook = Workbook(os.path.join(output_dir, name), {'strings_to_numbers': True, 'constant_memory': True})
    for csvFile in files:
        name = csvFile.replace('.csv', '').replace(output_dir + os.sep, '')
        worksheet = workbook.add_worksheet(name)
        with open(csvFile, 'r') as csvfile:
            csvreader = csv.reader(csvfile, delimiter=',')
            for row_index, row in enumerate(csvreader):
                for col_index, data in enumerate(row):
                    worksheet.write(row_index, col_index, data)
        # TODO: add chart
        # if name == 'summary':
        #     chart = workbook.add_chart({'type': 'column'})
        # chart.add_series({
        #     'name':
        # })
    workbook.close()


def parse(item):
    try:
        newItem = float(item)
        return newItem
    except ValueError:
        pass
    try:
        booleans = ["true", "false"]
        if item.lower() in booleans:
            newItem = bool(item)
            return newItem
    except ValueError:
        pass
    try:
        if item.startswith("b'"):
            newItem = item.replace("b'", '', 1)
            newItem = re.sub(r"(.*)'", r'\1', newItem)
            newItem = bytes(newItem, "ascii").decode("utf-8")
            # Not perfected yet
            # return newItem
    except (ValueError, TypeError):
        pass
    return item


if __name__ == "__main__":

    ''' Vamanos '''
    get_args()
