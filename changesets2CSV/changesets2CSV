#!/usr/bin/env python3
'''
changesets2CSV

This tool will build out a CSV of changeset info queried based on the given parameters

Copyright (c) 2019 Kaart Group <admin@kaartgroup.com>

Released under the MIT license: http://opensource.org/licenses/mit-license.php

'''

import xml.etree.ElementTree as ET
import csv
import argparse
import requests
from cachecontrol import CacheControl
from datetime import datetime, date, timedelta
import os
import sys
import glob
import re
import time
import ssl
from xlsxwriter.workbook import Workbook
import json

TEST_OVERPASS_USER_AGENT = "trackDownload/0.1 (lucas.bingham@kaartgroup.com)"
TEST_JSON_DICT = {'tags':[{'check':'name','const':'highway'}],'users':[{'user_id':'9320902','name':'Traaker_L'}]}
#,{'user_id':'8600365','name':'spuddy93'}

def valid_date(s):
    try:
        return datetime.strptime(s, "%Y-%m-%d")
    except ValueError:
        msg = "Not a valid date: '{0}'.".format(s)
        raise argparse.ArgumentTypeError(msg)


def create_specific(args):
    out = os.path.join(args.output, args.user + '.csv')
    changesets = get_changesets(args.user, args.start_time, args.end_time, args.bbox)
    if len(changesets) < 1:
        sys.exit(1)
    changeset_csv(out, changesets)
    if args.excel:
        create_excel_file


def create_summary(args):
    changeset_total = 0
    edit_total = 0
    disc_total = 0
    epc_total = 0  # edits per changeset
    add_total = 0
    mod_total = 0
    delete_total = 0
    summary = []

    with open(args.users, mode='r') as f:
        users = TEST_JSON_DICT['users']
        for user in users:
            print(user['name'])
            print(user['user_id'])

        for user in users:
            print("Processing changesets for {} ...".format(str(user['name'])), end="\r")
            out_file = os.path.join(args.output, str(user['name']) + '.csv')
            changesets = get_changesets(user['user_id'], args.start_time, args.end_time, args.bbox)

            # no point in doing anything if there aren't any changesets
            if len(changesets) < 1:
                print("Warning: There were no changesets for {} within those parameters\n".format(str(user['name'])))
                continue

            counts = changeset_csv(out_file, changesets, name=str(user['name']), summary=True)
            counts['Editor'] = str(user['name'])
            summary.append(counts)
            changeset_total += counts['Changesets']
            edit_total += counts['Edits']
            disc_total += counts['Discussions']
            add_total += counts['Additions']
            mod_total += counts['Modifications']
            delete_total += counts['Deletions']
            time.sleep(10)
            print("Processing changesets for {} ... Done\n".format(str(user['name'])))

    try:
        epc_total = round((edit_total / changeset_total), 2)


        summary.append({
            'Editor': 'TOTAL',
            'Changesets': changeset_total,
            'Edits': edit_total,
            'Discussions': disc_total,
            'Edits/Changeset': epc_total,
            'Additions': add_total,
            'Modifications': mod_total,
            'Deletions': delete_total
        })
        summary_csv(args, os.path.join(args.output, "summary.csv"), summary)
    except ZeroDivisionError:
       print("Warning: There were no changesets within those parameters")


def summary_csv(args, output_file, summary):
    with open(output_file, 'w') as f:
        fieldnames = ['Editor', 'Changesets', 'Edits', 'Discussions', 'Edits/Changeset', 'Additions', 'Modifications', 'Deletions']
        csv_writer = csv.DictWriter(f, fieldnames=fieldnames)
        csv_writer.writeheader()

        csv_writer.writerows(summary)

    if args.excel:
        create_excel_file(args, os.path.dirname(os.path.realpath(output_file)))


def create_weekly(args):
    today = date.today()
    weekday = today.weekday()
    start_delta = timedelta(days=weekday, weeks=1)
    start = today - start_delta
    end = start + timedelta(days=5)
    args.start_time = str(start)
    args.end_time = str(end)
    create_summary(args)

def count_new_modified_deleted(changeset):
    """Get the new, modified, or deleted objects in a changeset
    >>> count_new_modified_deleted(get_changesets(user="vorpalblade", start_time="2019-07-07", end_time="2019-07-08")[0].get('id'))
    {'Added': 129, 'Modified': 5, 'Deleted': 8}
    """
    api_url = "https://www.openstreetmap.org/api/0.6/changeset/{changeset}/download".format(changeset=changeset)
    session = CacheControl(requests.session())
    result = session.get(api_url).text
    root = ET.fromstring(result)
    newModifiedDeleted = {}
    newModifiedDeleted['Added'] = len(root.findall('create'))
    newModifiedDeleted['Modified'] = len(root.findall('modify'))
    newModifiedDeleted['Deleted'] = len(root.findall('delete'))
    return newModifiedDeleted

def overpass_status(api_status_url = "https://overpass-api.de/api/status"):
    """Get the overpass status -- this returns an int with the time to wait"""
    session = requests.session()
    session.headers.update({'User-Agent': TEST_OVERPASS_USER_AGENT})
    cached_session = CacheControl(session)
    response = cached_session.get(api_status_url)
    if (response.status_code != requests.codes.ok):
        raise ValueError("Bad Request: {}".format(api_status_url))
    parsed_response = {'wait_time': []}
    for i in response.text.splitlines():
        if "Connected as" in i:
            parsed_response['connected_as'] = i.split(":")[1].strip()
        elif "Current time" in i:
            parsed_response['current_time'] = i.split(":")[1].strip()
        elif "Rate limit" in i:
            parsed_response['rate_limit'] = int(i.split(":")[1].strip())
        elif "slots available now" in i:
            parsed_response['slots_available'] = int(i.split(" ")[0].strip())
        elif "Slot available after" in i:
            parsed_response['wait_time'].append(int(i.split(" ")[5]))
    if 'slots_available' not in parsed_response:
        parsed_response['slots_available'] = 0
    wait_time = 0
    if parsed_response['rate_limit'] - parsed_response['slots_available'] >= 2 and len(parsed_response['wait_time']) > 0:
        return max(parsed_response['wait_time'])
    return wait_time

def overpass_query(query):
    """Query the overpass servers. This may block for extended periods of time, depending upon the query"""

    session = requests.session()
    session.headers.update({'User-Agent': TEST_OVERPASS_USER_AGENT})
    cached_session = CacheControl(session)
    response = cached_session.post("http://overpass-api.de/api/interpreter", data={'data': query})
    wait_time = overpass_status()
    loop = 0
    while (wait_time > 0):
        time.sleep(wait_time)
        wait_time = overpass_status()
        loop += 1
    while (response.status_code == requests.codes.too_many_requests):
        time.sleep(10)
        response = cached_session.post("http://overpass-api.de/api/interpreter", data={'data': query})
    if (response.status_code != requests.codes.ok):
        print("Bad request")
        print(response.text)
        print(response.status_code)
        raise ValueError("Bad Request: {}".format(query))

    xml = response.text

    if (response.status_code != requests.codes.ok):
        raise ValueError("We got a bad response code of {} for {} which resulted in:\r\n{}".format(response.status_code, query, xml))
    content_type = response.headers.get('content-type')
    if content_type == 'application/osm3s+xml':
        return ET.ElementTree(ElementTree.fromstring(xml))
    elif content_type == 'application/json':
        return response.json()
    else:
        raise ValueError("Unexpected content type ({}) from the query: {}".format(content_type, query))

def count_tag_change(changesets,tags, osm_obj_type="*",const_tag="none"):
  #Testing variables
  print_version_lists = True
  object_limit_for_query=0
  print_query = False
  dont_run_query = False
  print_query_response = False
  dont_process_query = False
  #TODO: sort data of tag changes by changeset for column data in csv
  #changesets = {<some_id>:[<objects>]}
  objects_by_changeset = {}
  #new_ver_objects = []

  #Get all objects touched in each changeset
  for changeset in changesets:
      #New changeset in list
      objects_by_changeset[changeset] = [] #?
      #Request XML for each changeset
      api_url = "https://www.openstreetmap.org/api/0.6/changeset/{changeset}/download".format(changeset=changeset)
      dev_api_url = "https://master.apis.dev.openstreetmap.org/api/0.6/changeset/{changeset}/download".format(changeset=changeset)
      api_url = api_url
      session = CacheControl(requests.session())
      result = session.get(api_url).text
      root = ET.fromstring(result)

      #If we are looking for a constant tag
      if const_tag != "none":
          #Retrieve all objects that have the tags we're looking for
          objs_modified = root.findall("./modify/{osm_obj_type}/tag[@k='{const_tag}']..".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
          objs_created = root.findall("./create/{osm_obj_type}/tag[@k='{const_tag}']..".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
          objs_deleted = root.findall("./delete/{osm_obj_type}/tag[@k='{const_tag}']..".format(const_tag=const_tag,osm_obj_type=osm_obj_type))

          #Store each modified object's data in our list, new_ver_objects, as dictionaries
          for obj in objs_modified:
              this_obj = {"id":obj.attrib['id'],"version":int(obj.attrib['version'])}
              these_tags = obj.findall("tag")
              for this_tag in these_tags:
                  this_obj[this_tag.attrib['k']] = this_tag.attrib['v']

              objects_by_changeset[changeset].append(this_obj)

      #If we only care about the tag being changed
      else:
          #Retrieve all objects that have the tags we're looking for
          objs_modified = root.findall("./modify/{osm_obj_type}".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
          objs_created = root.findall("./create/{osm_obj_type}".format(const_tag=const_tag,osm_obj_type=osm_obj_type))
          objs_deleted = root.findall("./delete/{osm_obj_type}".format(const_tag=const_tag,osm_obj_type=osm_obj_type))

          #Store each modified object's data in our list, new_ver_objects, as dictionaries
          for obj in objs_modified:
              this_obj = {"id":obj.attrib['id'],"version":int(obj.attrib['version'])}
              these_tags = obj.findall("tag")
              for this_tag in these_tags:
                  this_obj[this_tag.attrib['k']] = this_tag.attrib['v']

              objects_by_changeset[changeset].append(this_obj)


  change_count_by_changeset = {}
  #print(objects_by_changeset)

  #Count number of objects per changeset
  for this_k, this_v in objects_by_changeset.items():
      change_count_by_changeset[this_k] = len(this_v)





  #4Testing: print objects and data in new_ver_objects list
  if print_version_lists:
      print("New_Ver: ")
      for obj in objects_by_changeset: print(objects_by_changeset[obj])
      print()

  #Build query to get previous versions of all objects in new_ver_objects
  #Start of Overpass Query
  query = "[out:json][timeout:25];"
  query_count = 0
  for this_set in objects_by_changeset:
      #Build each query part for each object
      if (query_count < object_limit_for_query or object_limit_for_query == 0):
          for obj in objects_by_changeset[this_set]:

              #Can this ever happen?
              if obj["version"] > 1 and (query_count < object_limit_for_query or object_limit_for_query == 0):
                  if object_limit_for_query != 0:
                      print("Object ",query_count+1," of ",object_limit_for_query)
                  #Thank you Taylor
                  query_part = "timeline({osm_obj_type}, {osm_id}, {prev_version}); for (t['created']) {{ retro(_.val) {{ {osm_obj_type}(id:{osm_id}); out meta;}} }}"\
                  .format(osm_obj_type = osm_obj_type, osm_id = obj["id"],prev_version = int(obj["version"])-1)
                  query += query_part
                  query_count += 1

  #4Testing: print out the query and/or response
  if print_query: print(query)

  if dont_run_query == False:
      #Submit the query
      query_json = overpass_query(query)

  if print_query_response:
      if dont_run_query:
          print("Cannot print query response if it was not run")
      else:
          print(query_json)


  if dont_process_query == False and dont_run_query == False:
      #Dictionaries of id, value, version
      old_ver_objects = []
      old_objects_by_changeset = {}
      #old_objects_by_changeset{<id>:[<objects>]}
      current_set_index = 0
      objects_added = 0

      old_objects_by_changeset[changesets[current_set_index]] = []

      #Iterate through query result
      for element in query_json["elements"]:
          these_tags = element['tags']
          #print("set is ", changesets[current_set_index])
          #TODO: Grab all tags from old version objects
          this_obj = {"id":element['id'],"version":element['version']}
          for key, value in element['tags'].items():
              this_obj[key] = value

          old_objects_by_changeset[changesets[current_set_index]].append(this_obj)
          objects_added += 1

          #Ensure that we add objects to the right changeset
          if objects_added == change_count_by_changeset[changesets[current_set_index]]:
              objects_added = 0
              if current_set_index + 1 <= len(changesets) - 1:
                  current_set_index += 1
                  old_objects_by_changeset[changesets[current_set_index]] = []

      #4Testing: Print list of old-version objects
      if print_version_lists:
          print("Old_Ver: ")
          for obj in old_objects_by_changeset: print(old_objects_by_changeset[obj])
          print()

      #See what values changed
      changes_by_changeset = {}
      #Initialize change counts for all sets
      for set, changes in old_objects_by_changeset.items():
          changes_by_changeset[set] = {'added':0,'modified':0,'deleted':0}

      for set,changes in old_objects_by_changeset.items():
          this_old_set = old_objects_by_changeset[set]
          this_new_set = objects_by_changeset[set]

          for i in range(len(this_old_set)):
              for this_tag in tags:
                  #TODO: iterate through tags to check
                  if this_old_set[i].get(this_tag,False):
                      old_value = this_old_set[i][this_tag]
                  else:
                      old_value = None

                  if this_new_set[i].get(this_tag,False):
                      new_value = this_new_set[i][this_tag]
                  else:
                      new_value = None


                  #print(old_value," became ",new_value)

                  if old_value == None:
                      if new_value != None:
                          if old_value != new_value:
                              print('add')
                              changes_by_changeset[set]['added'] += 1
                          else:
                              print(old_value," didn't change")
                  else:
                      if new_value != None:
                          if old_value != new_value:
                              print('change')
                              changes_by_changeset[set]['modified'] += 1
                          else:
                              print(old_value," didn't change")
                      else:
                          if old_value != new_value:
                              print('delete')
                              changes_by_changeset[set]['deleted'] += 1
                          else:
                              print(old_value," didn't change")

      return changes_by_changeset
  else:
      return {'added':0,'modified':0,'deleted':0}

def get_changesets(user=None, start_time=None, end_time=None, bbox=None):
    """Get the changesets for a user between start_time and end_time with no bbox
    >>> len(get_changesets(user="vorpalblade", start_time="2019-07-07", end_time="2019-07-08"))
    1
    >>> len(get_changesets(user="9019988", start_time="2018-11-05", end_time="2018-11-06"))
    149
    >>> len(get_changesets(user="vorpalblade", start_time=[datetime(2019, 7, 7)], end_time="2019-07-08"))
    1
    >>> len(get_changesets(user="9019988", start_time="2018-11-05", end_time=[datetime(2018, 11, 6)]))
    149
    >>> len(get_changesets(user="9019988", start_time=[datetime(2018, 11, 5)], end_time=[datetime(2018, 11, 6)]))
    149
    """
    query_params = {}
    if user:
        if user.isdigit():
            query_params['user'] = user
        else:
            query_params['display_name'] = user

    if start_time and end_time:
        if type(start_time) is list and len(start_time) == 1:
            start_time = start_time[0].strftime("%Y-%m-%d")
        if type(end_time) is list and len(end_time) == 1:
            end_time = end_time[0].strftime("%Y-%m-%d")
        query_params['time'] = ','.join([start_time, end_time])

    if bbox:
        query_params['bbox'] = ','.join(bbox)

    changesets = []

    try:
        api_url = "https://api.openstreetmap.org/api/0.6/changesets"
        session = CacheControl(requests.session())
        result = session.get(api_url, params=query_params)
        root = ET.fromstring(result.text)
        sets = root.findall('changeset')
        changesets.extend(sets)

        dateFormat = '%Y-%m-%dT%H:%M:%SZ'
        while len(sets) >= 100:
            new_end = datetime.strptime(sets[-1].get('closed_at'), dateFormat) - timedelta(0,5)
            new_end = new_end.strftime(dateFormat)
            start_time = "1970-01-01" if not start_time else start_time
            query_params['time'] = ','.join([start_time, new_end])
            result = session.get(api_url, params=query_params).text
            root = ET.fromstring(result)
            sets = root.findall('changeset')
            changesets.extend(sets)

    except Exception as e:
        print("Error with calling the API: " + str(e))

    return changesets


def changeset_csv(output_file, changesets, name=None, summary=False):
    try:
        with open(output_file, 'w') as f:
            fieldnames = ['Username', 'ID', 'Comment', 'Open', 'Created at', 'Closed at', 'Changes', 'Added', 'Modified', 'Deleted']
            #TODO: add fields for add/mod/del of tags to be checked in the "JSON"
            field_prefixes = []
            for tag in TEST_JSON_DICT['tags']:
                prefix = tag['check']
                fieldnames.extend([(prefix +' added'),(prefix+' modified'),(prefix+' deleted')])
                field_prefixes.append(prefix)
            #fieldnames.extend ['']
            fieldnames.extend(['Discussions', 'URL'])
            csv_writer = csv.DictWriter(f, fieldnames=fieldnames)
            url = "https://www.openstreetmap.org/changeset/{}"

            csv_writer.writeheader()

            # Initialize counters for summary
            changeset_count = 0
            edit_count = 0
            add_count = 0
            modify_count = 0
            delete_count = 0
            discussion_count = 0

            #Count additions, changes, and deletions that are names
            changeset_ids = []
            for set in changesets:
                changeset_ids.append(set.get('id'))
            #TODO: iterate through "JSON" to get tags that we want to check
            #{object:<object type>,check_tag:<tag>,const_tag:<tag>}
            name_changes = count_tag_change(changeset_ids,TEST_JSON_DICT['tags'][0]['check'], "way", TEST_JSON_DICT['tags'][0]['const'])
            #Does this need to populate a list as well to fill columns from?
            #print(name_changes)

            #TODO: iterate through name_changes to grab the name adds, modifies, and deletes for each changeset
            for item in changesets:
                changeset = {'Username': item.get('user'),
                             'ID': item.get('id'),
                             'Comment': '',
                             'Open': item.get('open'),
                             'Created at': item.get('created_at'),
                             'Closed at': item.get('closed_at'),
                             'Changes': item.get('changes_count'),
                             'Discussions': item.get('comments_count'),
                             'URL': url.format(item.get('id'))}

                for child in item:
                    if child.get('k') == 'comment':
                        changeset['Comment'] = child.get('v').encode('utf-8')

                #General adds, modifies, deletes
                changesetInformation = count_new_modified_deleted(changeset['ID'])
                for key in changesetInformation:
                    changeset[key] = changesetInformation[key]

                #Tag-specific adds, modifies, deletes
                #print(item.get('id'))
                these_tag_changes = name_changes[item.get('id')]
                for key in these_tag_changes:
                    key_string = field_prefixes[0] + ' ' + key
                    #print(key_string)
                    changeset[key_string] = these_tag_changes[key]



                changeset_count += 1
                add_count += changeset['Added']
                modify_count += changeset['Modified']
                delete_count += changeset['Deleted']
                edit_count += int(item.get('changes_count'))
                discussion_count += int(item.get('comments_count'))

                csv_writer.writerow(changeset)


    except Exception as e:
        print("Error creating csv {}".format(e))
        raise

    if summary:
        return {'Changesets': changeset_count,
                'Edits': edit_count,
                'Additions': add_count,
                'Modifications': modify_count,
                'Deletions': delete_count,
                'Discussions': discussion_count,
                'Edits/Changeset': round((edit_count/changeset_count), 2)}

    return True


def get_args():
    parser = argparse.ArgumentParser(usage='''changesets2CSV [-h] [-b min_lon min_lat max_lon max_lat] [-v] <command>
    ''', description="Commands for creating changeset CSV's")
    parser.add_argument('-t', '--test', help="Run tests to make certain that everything works", action='store_true')
    parser.add_argument('--bbox', nargs=4,
                        help="The bbox to query changesets. Values separated by spaces.",
                        metavar=('min_lon', 'min_lat', 'max_lon', 'max_lat'))
    # parser.add_argument('-v', '--verbose', action='store_true') #nothing to verbose yet
    parser.add_argument('-o', '--output', help="Location to create .csv files (default is current location)", default=os.getcwd())
    parser.add_argument('-x', '--excel', help="Create a .xlsx file.", action='store_true')
    parser.set_defaults(which='main')
    subparsers = parser.add_subparsers(title='commands')

    specific = subparsers.add_parser("specific", help='Specific query', description="Run query with specific parameters and create one output file.")
    specific.add_argument('-u', '--user', help="The OSM username or user id to use for the query (either username or user id, NOT both).")
    specific.add_argument('-s', '--start_time', nargs=1, type=valid_date, help="The start time of the window to query (YYYY-MM-DD).")
    specific.add_argument('-e', '--end_time', nargs=1, type=valid_date, help="The end time of the window to query (YYYY-MM-DD).")
    specific.set_defaults(func=create_specific, which="specific")

    summary = subparsers.add_parser("summary", help="Create a summary of changesets", description="Create a summary for a specified time range.")
    summary.add_argument('users', help="Path of the .config file of users.")
    summary.add_argument('-s', '--start_time', nargs=1, type=valid_date, help="The start time of the window to query (YYYY-MM-DD).")
    summary.add_argument('-e', '--end_time', nargs=1, type=valid_date, help="The end time of the window to query (YYYY-MM-DD).")
    summary.set_defaults(func=create_summary, which="summary")

    weekly = subparsers.add_parser("weekly", help="Create a weekly summary of changesets")
    weekly.add_argument('users', help="Path of the .config file of users.")
    weekly.set_defaults(func=create_weekly, which="weekly")

    args = parser.parse_args()

    try:
        if args.test:
            import doctest
            doctest.testmod()
            return
        if args.which == 'specific':
            if not ((args.user or args.bbox) or (args.start_time and args.end_time)):
                parser.error('No query parameters supplied: add --user or --bbox, or --start_time and --end_time.')

        args.func(args)

    except AttributeError:
        parser.print_help(sys.stderr)


def create_excel_file(args, output_dir):
    files = sorted(glob.glob(output_dir + os.sep + '*' + '.csv'))
    if args.start_time and args.end_time:
        name = "_".join([str(args.start_time), str(args.end_time)]) + '.xlsx'
    else:
        name = 'Kaart_activity.xlsx'
    workbook = Workbook(os.path.join(output_dir, name), {'strings_to_numbers': True, 'constant_memory': True})
    for csvFile in files:
        name = csvFile.replace('.csv', '').replace(output_dir + os.sep, '')
        worksheet = workbook.add_worksheet(name)
        with open(csvFile, 'r') as csvfile:
            csvreader = csv.reader(csvfile, delimiter=',')
            for row_index, row in enumerate(csvreader):
                for col_index, data in enumerate(row):
                    worksheet.write(row_index, col_index, data)
        # TODO: add chart
        # if name == 'summary':
        #     chart = workbook.add_chart({'type': 'column'})
        # chart.add_series({
        #     'name':
        # })
    workbook.close()


def parse(item):
    try:
        newItem = float(item)
        return newItem
    except ValueError:
        pass
    try:
        booleans = ["true", "false"]
        if item.lower() in booleans:
            newItem = bool(item)
            return newItem
    except ValueError:
        pass
    try:
        if item.startswith("b'"):
            newItem = item.replace("b'", '', 1)
            newItem = re.sub(r"(.*)'", r'\1', newItem)
            newItem = bytes(newItem, "ascii").decode("utf-8")
            # Not perfected yet
            # return newItem
    except (ValueError, TypeError):
        pass
    return item


if __name__ == "__main__":

    ''' Vamanos '''
    get_args()
